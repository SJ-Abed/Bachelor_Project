'''# -*- coding: utf-8 -*-
"""run models item base.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XQ8inbjuKBnwDYfjvJ2IWvrz_iL9LFXt
"""

from google.colab import drive
drive.mount('/content/drive')

cd '/content/drive/My Drive/BC_Project/'
'''
import os 
os.listdir()

# !pip install pycaret==2.3.10 markupsafe==2.0.1 pyyaml==5.4.1 -qq

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
'''import warnings
warnings.filterwarnings('ignore')'''
# from sklearn.decomposition import NMF
from sklearn.preprocessing import normalize
import matplotlib.pyplot as plt
# from pycaret.classification import *

from sklearn.model_selection import train_test_split
# from sklearn.model_selection import cross_val_score

# from sklearn.svm import SVC
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.ensemble import GradientBoostingClassifier
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.ensemble import AdaBoostClassifier
# import xgboost as xgb
# from xgboost import XGBClassifier

# from sklearn.model_selection import GridSearchCV


from sklearn import metrics #(confusion_matrix)(accuracy_score)(recall_score)(precision_score)(f1_score)(classification_report)

import seaborn as sns
from imblearn.over_sampling import SMOTE

X_res = pd.read_csv('X_res_v1.csv',index_col=0)
y_res = pd.read_csv('y_res_v1.csv',index_col=0)

train_validation_merge = pd.read_csv('train_validation_merge_v1.csv',index_col=0)
test_validation_merge = pd.read_csv('test_validation_merge_v1.csv',index_col=0)

customer_ids = test_validation_merge.drop_duplicates('cid').sort_values('cid')
X_tr,X_ts,y_tr,y_ts = train_test_split(customer_ids , customer_ids['reorder_label'],random_state = 42 ,test_size =0.3)
train_validation_merge = pd.concat([train_validation_merge,test_validation_merge[test_validation_merge.cid.isin(X_tr.cid)] ], ignore_index=True)
test_validation_merge  = test_validation_merge[test_validation_merge.cid.isin(X_ts.cid)]

columns = [
    'days_since_prior_order',
    'day_of_week',
    'user_reorder_rate',
    'user_unique_products', 'user_total_products', 'user_avg_cart_size',
    'days_between_orders',
    'user_reordered_products', 'user_reordered_products_ratio',
    'user_unique_cats', 'user_reordered_cats',
    'user_reordered_cats_ratio', 'user_unique_classes',
    'user_reordered_classes', 'user_reordered_classes_ratio',
    'product_week_reorder_rate', 'cat_week_reorder_rate',
    'class_week_reorder_rate', 'p_days_since_prior_order_reorder_rate',
    't_days_since_prior_order_reorder_rate',
    'c_days_since_prior_order_reorder_rate',
    'u_days_since_prior_order_reorder_rate',
    'u_p_days_since_prior_reorder_rate',
    'u_t_days_since_prior_reorder_rate',
    'u_c_days_since_prior_reorder_rate', 'product_reorder_rate',
    'p_reduced_feat_1', 'p_reduced_feat_2', 'p_reduced_feat_3',
    'u_p_order_rate', 'u_p_reorder_rate', 'u_p_orders_since_last',
    'u_t_orders_since_last', 'u_c_orders_since_last', 'max_streak', 'max_streak_cat','cat_reorder_rate', 'u_t_order_rate',
       'u_t_reorder_rate']

len(columns)

# columns2= columns+['reorder_label']

def feature_target(data):
    global columns
    train_validation_feature = data[columns]

    train_validation_target = data[['reorder_label']]
    return train_validation_feature, train_validation_target


from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, y_train = feature_target(train_validation_merge)
X_test, y_test = feature_target(test_validation_merge)

scaler = StandardScaler().fit(X_train)
X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=columns)


params = {'n_estimators': 300,
          'max_depth': 4,
          'min_samples_split': 5,
          'learning_rate': 0.01}

from imblearn.over_sampling import SMOTE 
sm = SMOTE(random_state=42)
# X_res, y_res = sm.fit_resample(X_train_scaled, y_train)

# X_res.to_csv('X_res_v1.csv')
# y_res.to_csv('y_res_v1.csv')

# globals().keys()

del customer_ids,X_tr,X_ts,y_tr,y_ts,X_train,GradientBoostingClassifier,RandomForestClassifier,SMOTE,scaler,sm,test_validation_merge,train_validation_merge, X_test

import seaborn as sns
from sklearn import metrics

def plot_classification_report(kind ,model_name, prob=0.5):
  if kind=='prob':
    def threshold(row):
      if row > prob:
          return 1
      return 0
    y_pred = y_df['y_test_prob'].apply(threshold)
    report = metrics.classification_report(y_df['y_test'], y_pred)
    print(report)
    # classes = ['class 1', 'class 2', 'class 3']
    report_path = f"CR_IB_tot\{model_name}.txt"
    text_file = open(report_path, "w")
    n = text_file.write(report)
    text_file.close()
    plt.figure(figsize=(8, 6), dpi=80)
    sns.heatmap(metrics.confusion_matrix(y_df['y_test'], y_df['y_test_prediction']), annot=True, fmt='g')
    plt.savefig(f'CR_IB_tot\{model_name}.png')
    plt.show()
  elif kind=='pred':
    report = metrics.classification_report(y_df['y_test'], y_df['y_test_prediction'])
    print(report)
    # classes = ['class 1', 'class 2', 'class 3']
    report_path = f"CR_IB_tot\{model_name}.txt"
    text_file = open(report_path, "w")
    n = text_file.write(report)
    text_file.close()
    plt.figure(figsize=(8, 6), dpi=80)
    sns.heatmap(metrics.confusion_matrix(y_df['y_test'], y_df['y_test_prediction']), annot=True, fmt='g')
    plt.savefig(f'CR_IB_tot\{model_name}.png')
    plt.show()
  else:
    print('Invalid kind')

# pycaret_df=X_res.__deepcopy__()
# pycaret_df['reorder_label'] = y_res
# pycaret_setup = setup(data = pycaret_train,target = 'reorder_label',normalize = True,
#             pca=False,silent = True,session_id = 3650, numeric_features =numeric_col)

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# Logistic regression

# logistic regression for feature importance
from sklearn.linear_model import LogisticRegression
# define the model
model = LogisticRegression()
# fit the model
start_time = datetime.now()
print("Training Started :")
model.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)
# get importance
importance = model.coef_[0]
# summarize feature importance
# for i,v in enumerate(importance):
# 	print('Feature: %s, Score:\t\t %.5f' % (columns[i],v))
#predict test data 
y_test_prediction = model.predict(X_test_scaled)
y_test_prob = model.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# train analysis
y_train_prediction = model.predict(X_train_scaled)
y_train_prob = model.predict_proba(X_train_scaled)
y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
                           'y_train_prob': y_train_prob[:, 1],
                           'y_train_prediction': y_train_prediction})

# plot feature importance
plt.figure(figsize=(11,7), dpi=80)
plt.subplots_adjust(wspace=0.6, hspace=0.6, left=0.1, bottom=0.42, right=0.96, top=0.96)
plt.bar(columns, abs(importance))
plt.xticks(rotation = 3*90)
plt.savefig('FI_IB_tot\FI_LogR2.JPEG', dpi=300)
plt.show()

plot_classification_report('prob','logR',0.8)

del LogisticRegression,model,importance,difference,end_time,start_time,y_df,y_test_prediction,y_train_prediction,n,report,report_path
# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# KNN
'''from sklearn.neighbors import KNeighborsClassifier
error_rate = []
# Might take some time
for i in range(1,40):
    print(i)
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_res,y_res)
    pred_i = knn.predict(X_test_scaled)
    error_rate.append(np.mean(pred_i != y_test))
  
plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

# permutation feature importance with knn for classification
from sklearn.inspection import permutation_importance
# define the model
model = KNeighborsClassifier(n_neighbors=10)
# fit the model
start_time = datetime.now()
print("Training Started :")
model.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)


# # perform permutation importance
# results = permutation_importance(model, X_res, y_res, scoring='accuracy')
# # get importance
# importance = results.importances_mean
# # summarize feature importance
# # for i,v in enumerate(importance):
# # 	print('Feature: %s, Score:\t\t %.5f' % (columns[i],v))


#predict test data 
y_test_prediction = model.predict(X_test_scaled)
y_test_prob = model.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# # train analysis
# y_train_prediction = model.predict(X_train_scaled)
# y_train_prob = model.predict_proba(X_train_scaled)
# y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
#                            'y_train_prob': y_train_prob[:, 1],
#                            'y_train_prediction': y_train_prediction})

# plot feature importance
plt.figure(figsize=(11,6), dpi=80)
plt.bar(columns, importance)
plt.xticks(rotation = 3*90)
plt.savefig('FI_IB_tot\FI_KNN.JPEG', dpi=300)
plt.show()

'''
# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# Naive Bayes

# permutation feature importance with knn for classification
from sklearn.inspection import permutation_importance
from sklearn.naive_bayes import GaussianNB
# define the model
model = GaussianNB()
# fit the model
start_time = datetime.now()
print("Training Started :")
model.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)

from sklearn.inspection import permutation_importance

# perform permutation importance
results = permutation_importance(model, X_res, y_res, scoring='accuracy')
# get importance
importance = results.importances_mean
# summarize feature importance
# for i,v in enumerate(importance):
# 	print('Feature: %s, Score:\t\t %.5f' % (columns[i],v))


#predict test data 
y_test_prediction = model.predict(X_test_scaled)
y_test_prob = model.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# train analysis
y_train_prediction = model.predict(X_train_scaled)
y_train_prob = model.predict_proba(X_train_scaled)
y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
                           'y_train_prob': y_train_prob[:, 1],
                           'y_train_prediction': y_train_prediction})

# plot feature importance
plt.figure(figsize=(11,6), dpi=80)
plt.subplots_adjust(wspace=0.6, hspace=0.6, left=0.1, bottom=0.48, right=0.96, top=0.96)
plt.bar(columns, abs(importance))
plt.xticks(rotation = 3*90)
plt.savefig('FI_IB_tot\FI_GNB2.JPEG', dpi=300)
plt.show()

plot_classification_report('prob','GNB',0.99)
del GaussianNB,model,importance,results,y_test_prediction,y_train_prediction,y_train_prob,y_test_prob

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# SVC

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
'''from sklearn.svm import SVC
svc =  SVC(kernel='linear')
# fit the model
start_time = datetime.now()
print("Training Started :")
svc.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)


#predict test data 
y_test_prediction = model.predict(X_test_scaled)
y_test_prob = model.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# train analysis
y_train_prediction = model.predict(X_train_scaled)
y_train_prob = model.predict_proba(X_train_scaled)
y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
                           'y_train_prob': y_train_prob[:, 1],
                           'y_train_prediction': y_train_prediction})

# perm_importance = permutation_importance(svc, X_res, y_res)

# feature_names = ['feature1', 'feature2', 'feature3', ...... ]
# features = np.array(feature_names)

# sorted_idx = perm_importance.importances_mean.argsort()
# plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])
# plt.xlabel("Permutation Importance")

plot_classification_report('pred')
'''
# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# Decision Tree
from sklearn.tree import DecisionTreeClassifier
# output Training Time
start_time = datetime.now()
print("Training Started :")
xgbc = DecisionTreeClassifier()
xgbc.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)

y_test_prediction = xgbc.predict(X_test_scaled)
y_test_prob = xgbc.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# train analysis
y_train_prediction = xgbc.predict(X_train_scaled)
y_train_prob = xgbc.predict_proba(X_train_scaled)
y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
                           'y_train_prob': y_train_prob[:, 1],
                           'y_train_prediction': y_train_prediction})
importance = xgbc.feature_importances_
# plot feature importance
plt.figure(figsize=(11,6), dpi=80)
plt.subplots_adjust(wspace=0.6, hspace=0.6, left=0.1, bottom=0.48, right=0.96, top=0.96)
plt.bar(columns, importance)
plt.xticks(rotation = 3*90)
plt.savefig('FI_IB_tot\FI_DT.JPEG', dpi=300)
plt.show()

plot_classification_report('pred','DT')

del DecisionTreeClassifier,xgbc,importance,y_test_prediction,y_train_prediction,y_train_prob,y_test_prob
# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# Random Forest
from sklearn.ensemble import  RandomForestClassifier
# output Training Time
start_time = datetime.now()
print("Training Started :")
xgbc = RandomForestClassifier()
xgbc.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)

y_test_prediction = xgbc.predict(X_test_scaled)
y_test_prob = xgbc.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# train analysis
y_train_prediction = xgbc.predict(X_train_scaled)
y_train_prob = xgbc.predict_proba(X_train_scaled)
y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
                           'y_train_prob': y_train_prob[:, 1],
                           'y_train_prediction': y_train_prediction})
importance = xgbc.feature_importances_
# plot feature importance
plt.figure(figsize=(11,6), dpi=80)
plt.subplots_adjust(wspace=0.6, hspace=0.6, left=0.1, bottom=0.48, right=0.96, top=0.96)
plt.bar(columns, importance)
plt.xticks(rotation = 3*90)
plt.savefig('FI_IB_tot\FI_RF.JPEG', dpi=300)
plt.show()
plot_classification_report('prob','RF',0.33)

del RandomForestClassifier,xgbc,importance,y_test_prediction,y_train_prediction,y_train_prob,y_test_prob

# import pickle
# filename = 'finalized_model_item_base_RF.sav'
# pickle.dump(xgbc, open(filename, 'wb'))

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# Extra Trees

from sklearn.ensemble import ExtraTreesClassifier
# output Training Time
start_time = datetime.now()
print("Training Started :")
xgbc = ExtraTreesClassifier()
xgbc.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)

y_test_prediction = xgbc.predict(X_test_scaled)
y_test_prob = xgbc.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# # train analysis
# y_train_prediction = xgbc.predict(X_train_scaled)
# y_train_prob = xgbc.predict_proba(X_train_scaled)
# y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
#                            'y_train_prob': y_train_prob[:, 1],
#                            'y_train_prediction': y_train_prediction})
importance = xgbc.feature_importances_
# plot feature importance
plt.figure(figsize=(11,6), dpi=80)
plt.subplots_adjust(wspace=0.6, hspace=0.6, left=0.1, bottom=0.48, right=0.96, top=0.96)
plt.bar(columns, importance)
plt.xticks(rotation = 3*90)
plt.savefig('FI_IB_tot\FI_ET.JPEG', dpi=300)
plt.show()

plot_classification_report('prob','ET',0.45)
del ExtraTreesClassifier,xgbc,importance,y_test_prediction,y_train_prediction,y_train_prob,y_test_prob
'''
# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier
# output Training Time
start_time = datetime.now()
print("Training Started :")
xgbc = GradientBoostingClassifier(**params)
xgbc.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)

y_test_prediction = xgbc.predict(X_test_scaled)
y_test_prob = xgbc.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# # train analysis
# y_train_prediction = xgbc.predict(X_train_scaled)
# y_train_prob = xgbc.predict_proba(X_train_scaled)
# y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
#                            'y_train_prob': y_train_prob[:, 1],
#                            'y_train_prediction': y_train_prediction})
importance = xgbc.feature_importances_
# plot feature importance
plt.figure(figsize=(11,6), dpi=80)
plt.bar(columns, importance)
plt.xticks(rotation = 3*90)
plt.savefig('FI_IB_tot\FI_GradB.JPEG', dpi=300)
plt.show()

plot_classification_report('pred','GradB')'''

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# AdaBoost
from sklearn.ensemble import AdaBoostClassifier
# output Training Time
start_time = datetime.now()
print("Training Started :")
xgbc = AdaBoostClassifier()
xgbc.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)

y_test_prediction = xgbc.predict(X_test_scaled)
y_test_prob = xgbc.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# # train analysis
# y_train_prediction = xgbc.predict(X_train_scaled)
# y_train_prob = xgbc.predict_proba(X_train_scaled)
# y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
#                            'y_train_prob': y_train_prob[:, 1],
#                            'y_train_prediction': y_train_prediction})
importance = xgbc.feature_importances_
# plot feature importance
plt.figure(figsize=(11,6), dpi=80)
plt.subplots_adjust(wspace=0.6, hspace=0.6, left=0.1, bottom=0.48, right=0.96, top=0.96)
plt.bar(columns, importance)
plt.xticks(rotation = 3*90)
plt.savefig('FI_IB_tot\FI_ADA.JPEG', dpi=300)
plt.show()

plot_classification_report('pred','ADA')
del AdaBoostClassifier,xgbc,importance,y_test_prediction,y_test_prob

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# XGBoost

# xgboost for feature importance on a classification problem
from xgboost import XGBClassifier
from matplotlib import pyplot
# output Training Time
start_time = datetime.now()
print("Training Started :")
model = XGBClassifier()
model.fit(X_res, y_res)
print("Training Completed ")
end_time = datetime.now()
difference = end_time - start_time
print(difference)

y_test_prediction = model.predict(X_test_scaled)
y_test_prob = model.predict_proba(X_test_scaled)
y_df = pd.DataFrame({'y_test': y_test['reorder_label'],
                     'y_test_prob': y_test_prob[:, 1],
                     'y_test_prediction': y_test_prediction})
# # train analysis
# y_train_prediction = model.predict(X_train_scaled)
# y_train_prob = model.predict_proba(X_train_scaled)
# y_train_df = pd.DataFrame({'y_train': y_train['reorder_label'],
#                            'y_train_prob': y_train_prob[:, 1],
#                            'y_train_prediction': y_train_prediction})
importance = model.feature_importances_
# plot feature importance
plt.figure(figsize=(11,6), dpi=80)
plt.subplots_adjust(wspace=0.6, hspace=0.6, left=0.1, bottom=0.48, right=0.96, top=0.96)
plt.bar(columns, importance)
plt.xticks(rotation = 3*90)
plt.savefig('FI_IB_tot\FI_XGB.JPEG', dpi=300)
plt.show()
plot_classification_report('prob','XGB',0.3)



del XGBClassifier,model,importance,y_test_prediction,y_test_prob




# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!DOCTYPE html><html><body align="right"><br><div style="direction:;ltr"><p style="background-color: #F7FFCA; color:black; border:1px solid black; border-radius: 10px; font-size:36px; line-height:1.8; font-family: byekan; text-align:justify; padding-left: 50px; padding-right: 50px; padding: 25px;" >
# End of Story<br><br><br><br><br><br><br><br><br><br>
